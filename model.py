# -*- coding: utf-8 -*-
"""Call Centre Text Summarizer Using LLMs with Hugging Face.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HrIM1ZcVlkqBaQZRV7M7SSdrDvu4ZB48

1. Install the required packages to  demonstrate summarization using Hugging Face
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install sacremoses==0.0.53 # Tokenizer and Detokenizer
# %pip install datasets #Install the dataset that model is already trained
# %pip install transformers #Install the transformer to capture depenedency between words in a sentence using self attention mechanism
# %pip install torch torchvision torchaudio # Install torch for Deep learning

""" 2. Import the necessary modules from the installed packages"""

from datasets import load_dataset
from transformers import pipeline

"""Import the load_dataset function from the datasets package, which enables us to load the dataset, and the pipeline function from the transformers package, which allows us to create a pipeline for text summarization"""

#loading the dataset
xsum_dataset = load_dataset(
    "xsum", #Extreme Summarization (XSum) Dataset.
    version="1.2.0",
    cache_dir='/Documents/Huggin_Face/data'
)  # Note: We specify cache_dir to use predownloaded data.
xsum_dataset  # The printed representation of this object shows the `num_rows` of each dataset split.

"""xsum dataset, which comprises a collection of BBC articles and summaries

3. Work with a smaller subset of the dataset. Example 10
"""

xsum_sample = xsum_dataset["train"].select(range(10))
display(xsum_sample.to_pandas())

"""4. Create a summarization pipeline using Hugging Face and perform summarization on a given text

1. You are creating a text summarization pipeline using the pipeline function.

2. The task parameter is set to "summarization," indicating that you want to perform text summarization using the pipeline.

3. You specify the model parameter as "t5-small." This sets the underlying model for text summarization. "t5-small" is a smaller version of the T5 (Text-to-Text Transfer Transformer) model developed by Google.

4. You set the min_length and max_length parameters to control the length of the generated summaries. Summaries will be truncated if they exceed the max_length and extended if they fall below the min_length.

5. You enable truncation using the truncation parameter. This option allows the model to truncate input texts if they exceed the maximum token limit of the model.

6. You provide a model_kwargs dictionary with a cache_dir parameter. This specifies the directory where predownloaded models are stored. This can help speed up the process if you've already downloaded the model to the specified directory.
"""

summarizer = pipeline(
    task="summarization",
    model="t5-small",
    min_length=20,
    max_length=40,
    truncation=True,
    model_kwargs={"cache_dir": '/Documents/Huggin_Face/'},
)  # Note: We specify cache_dir to use predownloaded models.

"""7. Summary for a given document using the created summarization pipeline"""

#Apply the summarization pipeline to the first document in the xsum_sample dataset.
#The pipeline generates a summary for the document based on the specified model and length constraints.
summarizer(xsum_sample["document"][0])

results = summarizer(xsum_sample["document"])

import pandas as pd

df = display(
    pd.DataFrame.from_dict(results)
    .rename({"summary_text": "generated_summary"}, axis=1)
    .join(pd.DataFrame.from_dict(xsum_sample))[
        ["generated_summary", "summary", "document"]
    ]
)

"""8. Generate a summary directly from user input"""

# Ask the user for input
input_text = input("Enter the text you want to summarize: ")

# Generate the summary
summary = summarizer(input_text, max_length=150, min_length=30, do_sample=False)[0]['summary_text']

bullet_points = summary.split(". ")

for point in bullet_points:

    print(f"- {point}")

# Print the generated summary
print("Summary:", summary)

"""You are generating a summary for an input_text using the summarizer pipeline with some specific settings:

1. max_length=150: This sets the maximum length of the generated summary to 150 tokens. The model will ensure that the summary does not exceed this length.

2. min_length=30: This sets the minimum length of the generated summary to 30 tokens. The model will avoid generating summaries that are shorter than this length.

3. do_sample=False: This parameter disables sampling during generation, which means that the model will produce a deterministic output rather than generating multiple possible variations of the summary.

4. The generated summary will be stored in the summary variable as a text string. You can then use the summary variable to access and work with the generated summary text.
"""

